---
title: "트롤리 딜레마"
emoji: "🚃"
category: "culture"
tags: ["윤리학", "철학", "도덕"]
difficulty: 1
storyType: "realStory"
characters:
  - id: foot
    name: "필리파 풋"
    emoji: "👩‍🏫"
  - id: participant
    name: "실험 참가자"
    emoji: "🧑"
images:
  hook: "moral dilemma trolley tracks"
  scene-1: "oxford philosophy 1967"
  scene-2: "autonomous car ethics"
pubDate: "2026-02-15"
---

<!-- step:cinematic-hook -->
폭주하는 전차.
앞에 5명, 옆 선로엔 1명.
**레버를 당기겠는가?**

<!-- step:scene -->
1967년,
옥스퍼드 대학교.
철학자 필리파 풋의
사고실험.

<!-- step:dialogue:foot -->
"당신은 레버를
잡고 있습니다.
어떻게 하시겠습니까?"

<!-- step:narration -->
참가자의 90%가
레버를 당긴다.

<!-- step:impact -->
**1명을 죽여 5명을 살린다.**

<!-- step:scene -->
변형 문제.

<!-- step:dialogue:foot -->
"이번엔 다리 위입니다.
뚱뚱한 사람을 밀면
전차가 멈춥니다."

<!-- step:narration -->
같은 결과:
1명 죽고 5명 산다.

<!-- step:scene -->
하지만 참가자의 90%가
**밀지 않는다.**

<!-- step:dialogue:participant -->
"그건... 살인 같은데요."

<!-- step:narration -->
왜?
레버는 간접적이고
미는 건 직접적이다.

<!-- step:scene -->
2016년, MIT.
자율주행차 윤리 설문.

<!-- step:narration -->
보행자 5명
vs 탑승자 1명.
누구를 살릴 것인가?

<!-- step:scene -->
230만 명이 답했다.
나라마다, 문화마다
달랐다.

<!-- step:reveal-title -->
🚃 트롤리 딜레마
정답 없는 도덕적 선택.
결과주의 vs 의무론.
AI 시대,
이 문제가 현실이 됐다.

<!-- step:outro -->
**기계는 어떻게 결정할까?**
테슬라는 이미
이 문제를 코드로
풀어야 합니다.
당신이라면?
