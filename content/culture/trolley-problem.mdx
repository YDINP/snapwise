---
title: "트롤리 딜레마"
emoji: "🚃"
category: "culture"
tags: ["윤리학", "철학", "도덕"]
difficulty: 1
storyType: "realStory"
characters:
  - id: foot
    name: "필리파 풋"
    emoji: "👩‍🏫"
  - id: participant
    name: "실험 참가자"
    emoji: "🧑"
images:
  hook: "moral dilemma trolley tracks"
  scene-1: "oxford philosophy 1967"
  scene-2: "autonomous car ethics"
pubDate: "2026-02-15"
---

<!-- step:cinematic-hook -->
폭주하는 전차.
앞에 5명, 옆 선로엔 1명.
**레버를 당기겠는가?**

<!-- step:scene -->
1967년,
옥스퍼드 대학교.
철학자 필리파 풋의
사고실험.

<!-- step:dialogue:foot -->
"당신은 레버를
잡고 있습니다.
어떻게 하시겠습니까?"

<!-- step:stat -->
레버 당기겠다 | 참가자 90%
필리파 풋(1967) 트롤리 사고실험 응답 결과 기준.

<!-- step:impact -->
**1명을 죽여 5명을 살린다.**

<!-- step:scene -->
변형 문제.

<!-- step:dialogue:foot -->
"이번엔 다리 위입니다.
뚱뚱한 사람을 밀면
전차가 멈춥니다."

<!-- step:vs -->
레버 당기기: 간접적 행동 → 90% 동의
|
사람 밀기: 직접적 행동 → 90% 거부

<!-- step:scene -->
하지만 참가자의 90%가
**밀지 않는다.**

<!-- step:dialogue:participant -->
"그건... 살인 같은데요."

<!-- step:narration -->
왜?
레버는 간접적이고
미는 건 직접적이다.

<!-- step:scene -->
2016년, MIT.
자율주행차 윤리 설문.

<!-- step:stat -->
MIT 자율주행 윤리 설문 | 230만 명 참여
MIT Moral Machine 프로젝트(2016) 글로벌 설문 기준.

<!-- step:scene -->
나라마다, 문화마다
답이 달랐다.

<!-- step:reveal-title -->
🚃 트롤리 딜레마
정답 없는 도덕적 선택.
결과주의 vs 의무론.
AI 시대,
이 문제가 현실이 됐다.

<!-- step:outro -->
**기계는 어떻게 결정할까?**
테슬라는 이미
이 문제를 코드로
풀어야 합니다.
당신이라면?
